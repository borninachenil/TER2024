La théorie BCM, la modification synaptique BCM, ou la règle BCM, du nom d'Elie Bienenstock, Leon Cooper et Paul Munro, est une théorie décrivant les changements d'adaptation neuronale dans le cerveau ou dans un réseau de neurones pendant un processus d'apprentissage. Elle a initialement postulé pour l'apprentissage dans le cortex visuel en 1981. Le modèle BCM propose un seuil mobile pour l'induction de la potentialisation à long terme (LTP) ou de la dépression à long terme (LTD), et indique que la plasticité synaptique est stabilisée par une adaptation dynamique de l'activité post-synaptique moyennée dans le temps.
Selon le modèle BCM, lorsqu'un neurone pré-synaptique se déclenche, les neurones post-synaptiques auront tendance à subir une LTP s'il est dans un état de haute activité (par exemple, se déclenche à haute fréquence et/ou a des concentrations internes élevées en calcium ), ou LTD s'il est dans un état d'activité plus faible (par exemple, déclenchement à basse fréquence, faibles concentrations internes de calcium). Cette théorie est souvent utilisée pour expliquer comment les neurones corticaux peuvent subir à la fois LTP ou LTD en fonction de différents protocoles de stimulation de conditionnement appliqués aux neurones pré-synaptiques (généralement stimulation haute fréquence, ou HFS, pour LTP, ou stimulation basse fréquence, LFS, pour LTD).

Développement
En 1949, Donald Hebb a proposé un mécanisme de travail pour la mémoire et l'adaptation informatique dans le cerveau maintenant appelé Règle de Hebb résumé par la maxime les neurones qui s'excitent ensemble, se connectent ensemble. Cette notion est fondamentale dans la compréhension moderne du cerveau en tant que réseau de neurones et, bien qu'elle ne soit pas universellement vraie, reste une bonne première approximation étayée par des décennies de preuves,.
Cependant, la règle de Hebb possède plusieurs défaut : 

elle n'a aucun mécanisme permettant aux connexions de s'affaiblir
et aucune limite supérieure pour la force qu'elles peuvent obtenir.En d'autres termes, le modèle est instable, à la fois théoriquement et informatiquement.
Des modifications ultérieures ont progressivement amélioré la règle de Hebb, la normalisant et permettant la décomposition des synapses, où aucune activité ou activité non synchronisée entre les neurones n'entraîne une perte de force de connexion. De nouvelles preuves biologiques ont porté cette activité à son apogée dans les années 1970, où les théoriciens ont formalisé diverses approximations dans la théorie, telles que l'utilisation de la fréquence de déclenchement au lieu du potentiel pour déterminer l'excitation des neurones, et l'hypothèse d'une intégration synaptique idéale et, plus important encore, linéaire. de signaux. C'est-à-dire qu'il n'y a pas de comportement inattendu dans l'ajout de courants d'entrée pour déterminer si une cellule se déclenchera ou non.
Ces approximations ont abouti à la forme de base de BCM ci-dessous en 1979, mais l'étape finale a pris la forme d'une analyse mathématique pour prouver la stabilité et d'une analyse informatique pour prouver l'applicabilité, aboutissant à l'article de Bienenstock, Cooper et Munro en 1982.
Depuis lors, des expériences ont montré des preuves du comportement du BCM à la fois dans le cortex visuel et dans l'hippocampe, ce dernier jouant un rôle important dans la formation et le stockage des souvenirs. Ces deux zones sont bien étudiées expérimentalement, mais la théorie et l'expérience doivent encore établir un comportement synaptique concluant dans d'autres zones du cerveau. Il a été proposé que dans le cervelet, la fibre parallèle à la synapse des cellules de Purkinje suit une "règle BCM inverse", ce qui signifie qu'au moment de l'activation des fibres parallèles, une concentration élevée de calcium dans la cellule de Purkinje entraîne une LTD, tandis qu'une plus faible la concentration donne LTP. En outre, la mise en œuvre biologique de la plasticité synaptique dans le BCM n'a pas encore été établie.

Théorie
La règle BCM de base prend la forme

dmj(t)dt=ϕ(c(t))dj(t)−ϵmj(t),{\displaystyle \,{\frac {dm_{j}(t)}{dt}}=\phi ({\textbf {c}}(t))d_{j}(t)-\epsilon m_{j}(t),}où:

mj{\displaystyle m_{j}} est le poids synaptique du j{\displaystyle j} ème synapse,
dj{\displaystyle d_{j}} est j{\displaystyle j} le courant d'entrée de la synapse,
c(t)=w(t)d(t)=∑jwj(t)dj(t){\displaystyle c(t)={\textbf {w}}(t){\textbf {d}}(t)=\sum _{j}w_{j}(t)d_{j}(t)} est le produit interne des poids et des courants d'entrée (somme pondérée des entrées),
ϕ(c){\displaystyle \phi (c)} est une fonction non linéaire. Cette fonction doit changer de signe à un certain seuil θM{\displaystyle \theta _{M}}, C'est, ϕ(c)<0{\displaystyle \phi (c)<0} si et seulement si c<θM{\displaystyle c<\theta _{M}}. Voir ci-dessous pour les détails et les propriétés.
et ϵ{\displaystyle \epsilon } est la constante de temps (souvent négligeable) de la décroissance uniforme de toutes les synapses.Ce modèle est une forme modifiée de la règle d'apprentissage Hebbian, wj˙=cdj{\displaystyle {\dot {w_{j}}}=cd_{j}}, et nécessite un choix de fonction adapté ϕ{\displaystyle \phi } pour éviter les problèmes hebbiens d'instabilité.
Bienenstock et al. réécrivirent ϕ(c){\displaystyle \phi (c)} en tant que fonction ϕ(c,c¯){\displaystyle \phi (c,{\bar {c}})} où c¯{\displaystyle {\bar {c}}} est la moyenne temporelle de c{\displaystyle c}. Avec cette modification et en écartant la décroissance uniforme, la règle prend la forme vectorielle :

m˙(t)=ϕ(c(t),c¯(t))d(t){\displaystyle {\dot {\mathbf {m} }}(t)=\phi (c(t),{\bar {c}}(t))\mathbf {d} (t)}Les conditions d'un apprentissage stable sont dérivées rigoureusement dans BCM en notant qu'avec c(t)=m(t)⋅d(t){\displaystyle c(t)={\textbf {m}}(t)\cdot {\textbf {d}}(t)} et avec l'approximation de la production moyenne c¯(t)≈m(t)d¯{\displaystyle {\bar {c}}(t)\approx {\textbf {m}}(t){\bar {\mathbf {d} }}}, il suffit que

sgn⁡ϕ(c,c¯)=sgn⁡(c−(c¯c0)pc¯)  for c>0, and{\displaystyle \,\operatorname {sgn} \phi (c,{\bar {c}})=\operatorname {sgn} \left(c-\left({\frac {\bar {c}}{c_{0}}}\right)^{p}{\bar {c}}\right)~~{\textrm {for}}~c>0,~{\textrm {and}}}
ϕ(0,c¯)=0  for all c¯,{\displaystyle \,\phi (0,{\bar {c}})=0~~{\textrm {for}}~{\textrm {all}}~{\bar {c}},}ou de manière équivalente, que le seuil θM(c¯)=(c¯/c0)pc¯{\displaystyle \theta _{M}({\bar {c}})=({\bar {c}}/c_{0})^{p}{\bar {c}}}, où p{\displaystyle p} et c0{\displaystyle c_{0}} sont des constantes positives fixées.
Lorsqu'elle est mise en œuvre, la théorie est souvent prise de telle sorte que

ϕ(c,c¯)=c(c−θM)   and   θM=c¯2=1τ∫−∞tc2(t′)e−(t−t′)/τdt′,{\displaystyle \,\phi (c,{\bar {c}})=c(c-\theta _{M})~~~{\textrm {and}}~~~\theta _{M}={\bar {c}}^{2}={\frac {1}{\tau }}\int _{-\infty }^{t}c^{2}(t^{\prime })e^{-(t-t^{\prime })/\tau }dt^{\prime },}où τ{\displaystyle \tau } est une constante de temps de sélectivité.
Le modèle présente des inconvénients, car il nécessite à la fois une potentialisation à long terme et une dépression à long terme, ou des augmentations et des diminutions de la force synaptique, ce qui n'a pas été observé dans tous les systèmes corticaux. De plus, il nécessite un seuil d'activation variable et dépend fortement de la stabilité des points fixes sélectionnés. c0{\displaystyle c_{0}} et p{\displaystyle p}. Cependant, la force du modèle est qu'il intègre toutes ces exigences à partir de règles de stabilité dérivées indépendamment, telles que la normalisabilité et une fonction de décroissance avec le temps proportionnel au carré de la sortie .

Exemple
Cet exemple est un cas particulier de celui du chapitre "Résultats mathématiques" de Bienenstock et al. en supposant p=2{\displaystyle p=2} et c0=1{\displaystyle c_{0}=1}. Avec ces valeurs θM=(c¯/c0)pc¯=c¯3{\displaystyle \theta _{M}=({\bar {c}}/c_{0})^{p}{\bar {c}}={\bar {c}}^{3}} et nous décidons ϕ(c,c¯)=c(c−θM){\displaystyle \phi (c,{\bar {c}})=c(c-\theta _{M})} remplissant les conditions de stabilité énoncées au chapitre précédent.
Supposons que deux neurones présynaptiques fournissent des entrées d1{\displaystyle d_{1}} et d2{\displaystyle d_{2}}, son activité un cycle répétitif avec la moitié du temps d=(d1,d2)=(0.9,0.1){\displaystyle \mathbf {d} =(d_{1},d_{2})=(0.9,0.1)} et le temps restant d=(0.2,0.7){\displaystyle \mathbf {d} =(0.2,0.7)}. c¯{\displaystyle {\bar {c}}} la moyenne temporelle sera la moyenne de c{\displaystyle c} valeur dans la première et la seconde moitié d'un cycle.
Soit la valeur initiale des poids m=(0.1,0.05){\displaystyle \mathbf {m} =(0.1,0.05)}. Dans la première moitié du temps d=(0.9,0.1){\displaystyle \mathbf {d} =(0.9,0.1)} et m=(0.1,0.05){\displaystyle \mathbf {m} =(0.1,0.05)}, la somme pondérée c{\displaystyle c} est égal à 0,095 et nous utilisons la même valeur que la moyenne initiale c¯{\displaystyle {\bar {c}}}. Cela signifie θM=0.001{\displaystyle \theta _{M}=0.001}, ϕ=0.009{\displaystyle \phi =0.009}, m˙=(0.008,0.001){\displaystyle {\dot {m}}=(0.008,0.001)}. En ajoutant 10% de la dérivée aux poids, nous en obtenons de nouveaux m=(0.101,0.051){\displaystyle \mathbf {m} =(0.101,0.051)}.
Dans la prochaine moitié du temps, les entrées sont d=(0.2,0.7){\displaystyle \mathbf {d} =(0.2,0.7)} et poids m=(0.101,0.051){\displaystyle \mathbf {m} =(0.101,0.051)}. Cela signifie c=0.055{\displaystyle c=0.055}, c¯{\displaystyle {\bar {c}}} du cycle complet est de 0,075, θM=0.000{\displaystyle \theta _{M}=0.000}, ϕ=0.003{\displaystyle \phi =0.003}, m˙=(0.001,0.002){\displaystyle {\dot {m}}=(0.001,0.002)}. En ajoutant 10% de la dérivée aux poids, nous en obtenons de nouveaux m=(0.110,0.055){\displaystyle \mathbf {m} =(0.110,0.055)}.
En répétant le cycle précédent on obtient, après plusieurs centaines d'itérations, que la stabilité est atteinte avec m=(3.246,−0.927){\displaystyle \mathbf {m} =(3.246,-0.927)}, c=8=2.828{\displaystyle c={\sqrt {8}}=2.828} (première mi-temps) et c=0.000{\displaystyle c=0.000} (temps restant), c¯=8/2=1.414{\displaystyle {\bar {c}}={\sqrt {8}}/2=1.414}, θM=8=2.828{\displaystyle \theta _{M}={\sqrt {8}}=2.828}, ϕ=0.000{\displaystyle \phi =0.000} et m˙=(0.000,0.000){\displaystyle {\dot {m}}=(0.000,0.000)}.
Notez comment, comme prévu, le vecteur de poids final m{\displaystyle m} est devenu orthogonal à l'un des modèles d'entrée, étant les valeurs finales de c{\displaystyle c} dans les deux intervalles zéros de la fonction ϕ{\displaystyle \phi }.

Expérience
La première confirmation expérimentale majeure de BCM est venue en 1992 en étudiant LTP et LTD dans l'hippocampe. Le travail expérimental de Serena Dudek a montré un accord qualitatif avec la forme finale de la fonction d'activation du BCM. Cette expérience a ensuite été reproduite dans le cortex visuel, que le BCM a été initialement conçu pour modéliser. Ce travail a fourni une preuve supplémentaire de la nécessité d'une fonction de seuil variable pour la stabilité dans l'apprentissage de type Hebbian (BCM ou autres).
Les preuves expérimentales n'étaient pas spécifiques au BCM jusqu'à ce que Rittenhouse et al. a confirmé la prédiction de BCM de la modification des synapses dans le cortex visuel lorsqu'un œil est sélectivement fermé. Spécifiquement,

log⁡(mclosed(t)mclosed(0))∼−n2¯t,{\displaystyle \log \left({\frac {m_{\rm {closed}}(t)}{m_{\rm {closed}}(0)}}\right)\sim -{\overline {n^{2}}}t,}où n2¯{\displaystyle {\overline {n^{2}}}} décrit la variance de l'activité spontanée ou du bruit dans l'œil fermé et t{\displaystyle t} est le temps écoulé depuis la fermeture. L'expérience a confirmé la forme générale de cette prédiction et a fourni une explication de la dynamique de la fermeture de l'œil monoculaire (privation monoculaire ) par rapport à la fermeture de l'œil binoculaire. Les résultats expérimentaux sont loin d'être concluants, mais jusqu'à présent ont favorisé le BCM par rapport aux théories concurrentes de la plasticité.

Applications
Alors que l'algorithme de BCM est trop compliqué pour le traitement distribué parallèle à grande échelle, il a été utilisé dans les réseaux latéraux avec un certain succès. De plus, certains algorithmes d'apprentissage de réseau informatique existants ont été conçus pour correspondre à l'apprentissage BCM .

Références
Voir aussi
Articles connexes
Règle de Hebb
Potentialisation à long terme
Dépression synaptique à long terme
Plasticité synaptique

Liens externes
Article de Scholarpedia Portail de l’éducation   Portail des neurosciences